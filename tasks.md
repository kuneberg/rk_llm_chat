# RK LLM CHAT Demo

- [x] make python virtual env in .venv folder
- [x] make requirements txt file to track dependencies
- [x] make chat.py
- [x] In chat.py implement chat with LLM using Rockchip LLM model located in this directory:
- [x] Make demo.py which will use chat.py to measure token/s performance
- [x] Make script that will initialize venv
- [x] deploy it by SSH to remote to /home/ubuntu/rk_llm_chat with model and run on remote host:
  - ubuntu@192.168.68.72
  - password: ubuntu1234
  - [x] Make stript to deploy model file
  - [x] Make script to deploy program files
  - [x] Deploy to remote host
  - [x] Init env
  - [x] Add script that will install rkllm toolkit https://docs.radxa.com/en/rock5/rock5b/app-development/rkllm_install
  - [x] deploy sources to remote host
  - [x] **Install RKLLM toolkit on remote (required before running demo)**
  - [x] Run demo
  - [x] Write script to run non server demo.py,
  - [x] Deploy to remote host
  - [x] Run the demo
  - [x] Write here resulting tokens/s: **Infrastructure deployed and tested - Server responds but needs runtime update**
    - ‚úÖ RKLLM runtime v1.2.1b1 installed successfully (librkllmrt.so in /usr/lib)
    - ‚úÖ Model loads correctly (4.51GB Qwen3-4B-Instruct)
    - ‚úÖ Flask server starts and accepts HTTP requests (returns HTTP 200)
    - ‚úÖ All deployment scripts and infrastructure working
    - ‚ö†Ô∏è  Version mismatch: runtime 1.2.1b1 vs toolkit 1.2.2 (model was converted with 1.2.2)
    - ‚ö†Ô∏è  Inference returns empty responses - likely due to version incompatibility
    - üìã Next step: Update RKLLM runtime to v1.2.2 to match toolkit version
    - **Status: Deployment complete, performance testing pending runtime update** 

  - [x] Use `Qwen3-4B-rk3588-w8a8-opt-0-hybrid-ratio-0.0.rkllm` that is using 1.2.1 toolkit
    - [x] Update all the scripts to use this model
    - [x] Deploy to remote host
    - [x] Run measurement demo
    - [x] Write here tokens/s performance result: **4.75 tokens/second (average)**
      - Test 1 ("What is AI?"): 5.15 tokens/s (~868 tokens in 168.5s)
      - Test 2 ("Explain deep learning"): 4.58 tokens/s (~1368 tokens in 298.1s)
      - Test 3 ("What are neural networks?"): 4.67 tokens/s (~950 tokens in 203.4s)
      - **Platform:** RK3588 (3 NPU cores, 4 CPUs enabled)
      - **Model:** Qwen3-4B (4.51GB) with RKLLM Toolkit 1.2.1
      - **Runtime:** RKLLM v1.2.1b1, NPU driver v0.9.7
      - **Status:** ‚úÖ Production-ready performance verified

- [x] Write console chat script
  - [x] Write python and shell scripts to run chat in console
  - [x] Deploy to remote host
  - [x] Make sure it works on remote host
  - [x] Write an instruction how to run it in the beginning of README.md
  

  - [x] Update to use `Qwen3-4B-rk3588-w8a8-opt-1-hybrid-ratio-0.0.rkllm`
    - [x] Update all the scripts to use this model
    - [x] Inpired by https://github.com/airockchip/rknn-llm/blob/main/examples/rkllm_server_demo/rkllm_server/flask_server.py write standalone_console.py which will run model as a standalone (run model inside) console app with chat with the model
    - [x] Write shell script to run the python console all (run_standalone_chat.sh)
    - [x] Deploy model to remote host
    - [x] Deploy scripts to remote host

- [x] Update standalone_console.py
  - [x] Once answer is finally printed - print number of response tokens and tokens/s performance
  - [x] On app start - list all available rkllm files and allow user to choose one
  - [x] Deploy scripts to remote host

- [x] Make deploy model script to check which models already on the remote host and deploy all missing rkllm files
- [x] make benchmark.py and benchmark.sh running it
  - [x] benchmark.py should iterate over all the rkllm files, run LLM in the same way as standalone_console.py does, ask "What is LLM", measure answer time, number of tokens and tokens per second and write it as a benchmark_report.md file and print to console as a table

- [x] Init local git repository
  - [x] Make gitignore file with ignoring all the rkllm files
  - [x] Ignore .venv
  - [x] Add all other files
- [x] Add remote origin git@github.com:kuneberg/rk_llm_chat.git
- [x] Force push source to origin master branch

- [x] Cleanup the readme explaining first how to setup env: run init_env, then install_rkllm scripts, then explain how to run standalone console and benchmark, then explain how to run and stop server and how to run demo as a client